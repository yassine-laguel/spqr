

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>&lt;no title&gt; &mdash; SPQR  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> SPQR
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Table of Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="start.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_detailed/oracles.html">API Oracles</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_detailed/algorithms.html">API Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_detailed/risk_optimization.html">API Optimization Framework</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">SPQR</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>&lt;no title&gt;</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/toolbox_demonstration.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 5ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<table><tr style="background-color:#FFFFFF;"><td width=15%><table><tr></tr><tr></tr></table></td><td><center><h1><p>Supyquantile : A Toolbox for Minimization of Superquantile-Based Risk Measures</p>
</h1></center></td><td width=15%><table><tr><p>Yassine Laguel</p>
</tr><tr><p>Jerôme Malick</p>
</tr><tr><p>Zaid Harchaoui</p>
</tr></table></td></tr></table><div id="top"></div><center><p>The Toolbox</p>
</center><div id="part1_1"></div><p>A Generic Problem</p>
<p>Supyquantile is a python toolbox which is aimed at providing optimization first order convex algorithms for the minimization of superquantile based measures. In practice, it proposes to solve problems of the form :</p>
<div class="math notranslate nohighlight">
\[\min_{w \in \mathbb{R}^n}  Cvar(\hat{L}(w))\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{L} : w \mapsto (L(w,x_i,y_i))_{(1 \leq i \leq n)}\)</span> takes an input <span class="math notranslate nohighlight">\(w \in \mathbb{R}^d\)</span> and maps it to a discrete random variable composed of n outcomes.</p>
<p>Here the couples <span class="math notranslate nohighlight">\((x_i, y_i) \in \mathbb{R}^d \times \mathbb{R}\)</span> refer to the data provided by the user and associated to the problem. The function $L :<span class="math">\mathbb{R}`^d :nbsphinx-math:</span>times <cite>:nbsphinx-math:</cite>mathbb{R}`^d <span class="math">\times `:nbsphinx-math:</span>mathbb{R}` <span class="math">\rightarrow `:nbsphinx-math:</span>mathbb{R}` <span class="math notranslate nohighlight">\(, also provided by the user is assumed to satisfy at least : &lt;ul&gt;  &lt;li&gt;\)</span>L$ is convex with respect to <span class="math notranslate nohighlight">\(w\)</span></p>
</li><div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&lt;li&gt;$L$ is subdifferentiable with respect to $w$&lt;/li&gt;
</pre></div>
</div>
</ul><p>We show below two instances of this problem, Quantile Regression and Distributionally Robust Least Squares and how our toolbox can treat them.</p>
<div id="part1_2"></div><p>Functionning</p>
<p>The user can import toolbox’s solver with statement :</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s1">&#39;..&#39;</span><span class="p">))</span>

<span class="kn">from</span> <span class="nn">spqr.risk_optimization</span> <span class="k">import</span> <span class="n">RiskOptimizer</span>
</pre></div>
</div>
</div>
<p>To create an instance of this solver, all the user needs is to provide a function <span class="math notranslate nohighlight">\(L\)</span> as well as well as its subgradient (or gradient).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">L</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">L_prime</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">))</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">RiskOptimizer</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">L_prime</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Supyquantile inherits from scikit-learn estimator’s class. Thus, it provides a fit(), a predict() and a score() method that work as for usual scikit-learn estimators:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">make_regression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>

<span class="c1"># generate regression dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>


<span class="n">optimizer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Algorithm Score : &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Algorithm Score : 8264.192448074133
</pre></div></div>
</div>
<p>Iterates of the algorithm and solution after fitting are available through :</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">list_iterates</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">list_iterates</span>
<span class="n">solution</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">solution</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Solution found :&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">solution</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Solution found :[-1.34952653e-12 -1.34952653e-12]
</pre></div></div>
</div>
<p>The function <span class="math notranslate nohighlight">\(Cvar\circ L\)</span> or its gradient at a given point <span class="math notranslate nohighlight">\(w\)</span>, one can use :</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">w</span> <span class="o">=</span> <span class="n">solution</span>
<span class="n">value</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">oracle</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">gradient</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">oracle</span><span class="o">.</span><span class="n">g</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Value found : &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Gradient found : &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">gradient</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Value found : 164.93406592062163
Gradient found : 0.2238805970149254
</pre></div></div>
</div>
<div id="part1_2"></div><p>Custom Parameterization</p>
<p>The class RiskOptimizer can be parametrized in two different ways.</p>
<p>First option is to directly precise specific parameters wanted at instanciation of the class :</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">RiskOptimizer</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">L_prime</span><span class="p">,</span><span class="n">p</span><span class="o">=</span><span class="mf">0.72</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Customizable parameters are :</p>
<ul><li><p>p: the probability level chosen for Cvar (by default=0.8)</p>
</li><li><p>algorithm: the chosen first order method (by default=‘subgradient’)</p>
</li><li><p>w_start: the starting point chosen for the iterative method (by default=<span class="math notranslate nohighlight">\(0_{\mathbb{R}^d}\)</span>)</p>
</li><li><p>alpha: constant involved in the learning rate adopted</p>
</li><li><p>nb_iterations: number of iterations wished for the algorithm</p>
</li></ul><p>Second option is to provide a dictionary containing the specified parameters</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># General Parameters</span>
    <span class="s1">&#39;algorithm&#39;</span><span class="p">:</span> <span class="s1">&#39;catalyst&#39;</span><span class="p">,</span>
    <span class="s1">&#39;w_start&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s1">&#39;p&#39;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span>
    <span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="c1"># Catalyst Parameters</span>
    <span class="s1">&#39;catalyst_nb_iterations&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
    <span class="s1">&#39;catalyst_kappa&#39;</span><span class="p">:</span> <span class="mi">100</span>
    <span class="p">}</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">RiskOptimizer</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">L_prime</span><span class="p">,</span><span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div id="part2"></div><p>Application Examples</p>
<div id="part2_1"></div><center><p>&lt;a style=“font-size: 15pt; font-weight: bold”; id=“#quantile_regression”&gt; Quantile Regression</p>
</center><p style="text-align: right; font-size: 10px;"><p>Go to top</p>
</p><p>Quantile regression offers a linear model to approximate the <span class="math notranslate nohighlight">\(p\)</span>-quantile of a random variable <span class="math notranslate nohighlight">\(Y\)</span>, given the information brought by some features <span class="math notranslate nohighlight">\(X_1,\dots, X_d\)</span>.</p>
<p>Let us first build some instance of dataset :</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Build a dataset</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="n">datasize</span> <span class="o">=</span> <span class="mi">800</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">7.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">])</span>
<span class="n">bias</span> <span class="o">=</span> <span class="mf">3.0</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">40.0</span><span class="p">,</span><span class="mf">60.0</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">datasize</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span><span class="o">.</span><span class="n">T</span> <span class="c1"># Here we use two features : x and x^2 to</span>
                                  <span class="c1"># provide a quadratic approximation of the desired quantile</span>

<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">2000.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">datasize</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">bias</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">datasize</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">alpha</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/toolbox_demonstration_19_0.png" src="_images/toolbox_demonstration_19_0.png" />
</div>
</div>
<p>It can be shown (see Rockafellar et al.) that a couple <span class="math notranslate nohighlight">\(w_0^\star,w^\star\)</span> such that <span class="math notranslate nohighlight">\(w_0^\star + {w^\star}^T X\)</span> approximates the <span class="math notranslate nohighlight">\(p\)</span>-quantile of <span class="math notranslate nohighlight">\(Y\)</span> can be found by solving :</p>
<div class="math notranslate nohighlight">
\[w^\star = \textit{arg}min_{w \in \mathbb{R}^d} Cvar(y - w^T X) - \mathbb{E}[y - w^T X]\]</div>
<div class="math notranslate nohighlight">
\[w_0^\star = Q_p(Y - w^tX)\]</div>
<p>where <span class="math notranslate nohighlight">\(Q_p(Y - {w^\star}^T X)\)</span> refers to the <span class="math notranslate nohighlight">\(p\)</span>-quantile of <span class="math notranslate nohighlight">\(Y - {w^\star}^TX\)</span></p>
<p>If we introduce the function, <span class="math notranslate nohighlight">\(L : w,x,y \mapsto y - w^T X - (\mathbb{E}[y] - w^T \mathbb{E}[x])\)</span>, it is not difficult to show that we get then <span class="math notranslate nohighlight">\(w^\star = \textit{arg}min_{w \in \mathbb{R}^d} Cvar(\hat{L}(w))\)</span></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">mean_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">mean_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">L</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">y</span><span class="o">-</span><span class="n">mean_y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean_X</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">L_prime</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">mean_X</span> <span class="o">-</span> <span class="n">x</span>
</pre></div>
</div>
</div>
<p>This is all we need to launch the solver with desired probability <span class="math notranslate nohighlight">\(p\)</span></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">p</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">quantile_regressor</span> <span class="o">=</span> <span class="n">RiskOptimizer</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">L_prime</span><span class="p">,</span><span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
<span class="n">quantile_regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We can recover then the solution and compute the bias term <span class="math notranslate nohighlight">\(w_0^\star\)</span></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">supyquantile.measures</span> <span class="k">import</span> <span class="n">quantile</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">scale</span>

<span class="n">p</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">w_star</span> <span class="o">=</span> <span class="n">quantile_regressor</span><span class="o">.</span><span class="n">algorithm</span><span class="o">.</span><span class="n">w</span>
<span class="n">w_0</span> <span class="o">=</span> <span class="n">quantile</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">w_star</span><span class="p">))</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">w_0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">datasize</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">w_star</span><span class="p">)</span>

<span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">index</span><span class="p">],</span><span class="n">y_pred</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span><span class="s1">&#39;-quantile approximation&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/toolbox_demonstration_25_0.png" src="_images/toolbox_demonstration_25_0.png" />
</div>
</div>
<div id="part3"></div><center><p>&lt;a style=“font-size: 15pt; font-weight: bold”; id=“#safe_least_squares”&gt; Distributionally Robust Least Squares</p>
</center><p style="text-align: right; font-size: 10px;"><p>Go to top</p>
</p><p>With Superquantile Robust Linear Regression, we try to solve the problem</p>
<div class="math notranslate nohighlight">
\[\min_{w \in \mathbb{R}^n}  Cvar(||Y - w^T X||^2) + \frac{\lambda}{2} ||w||^2\]</div>
<p>With this model, we are trying to minimize the highest quantiles of the loss <span class="math notranslate nohighlight">\(||Y - w^T X||^2\)</span>, seen as a random variable depending on both <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>Let us first build some testing set for that purpose :</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">100.0</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">preprocess_dataset</span><span class="p">(</span><span class="n">x_set</span><span class="p">):</span>
    <span class="n">x_set</span> <span class="o">=</span> <span class="n">scale</span><span class="p">(</span><span class="n">x_set</span><span class="p">)</span>
    <span class="n">x_with_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">x_set</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_set</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">x_with_bias</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_set</span>
    <span class="k">return</span> <span class="n">x_with_bias</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">preprocess_dataset</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">preprocess_dataset</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We can build now our model :</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">lmbda</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="k">def</span> <span class="nf">L</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">lmbda</span><span class="o">/</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">w</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">L_prime</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">w2</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">lmbda</span> <span class="o">*</span> <span class="n">w2</span>

<span class="n">regressor_safe</span> <span class="o">=</span> <span class="n">RiskOptimizer</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">L_prime</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s1">&#39;bfgs&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We fit it :</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">regressor_safe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>and observe the associated score on a testing set :</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">regressor_safe</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Score for Robust Linear Prediction &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">regressor_safe</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Score for Robust Linear Prediction 11835.649032653691
</pre></div></div>
</div>
<p>A quick comparison with usual least squares gives :</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">Ridge</span>

<span class="n">ridge_regressor_l_2</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">lmbda</span><span class="o">/</span><span class="mf">2.0</span><span class="p">,</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ridge_regressor_l_2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">ground_y_pred</span> <span class="o">=</span> <span class="n">ridge_regressor_l_2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Score for Sklearn Ridge Regression &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">regressor_safe</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">ground_y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Score for Sklearn Ridge Regression 11599.036896260903
</pre></div></div>
</div>
<p>Obviously, Robust Linear Regression it is not assumed to perform systematically better than Ridge Regression.</p>
<div id="References"></div><p>References</p>
<ul><li id="#Rock"><p>ROCKAFELLAR, R. Tyrrell et ROYSET, Johannes O. Superquantiles and their applications to risk, random variables, and regression. In : Theory Driven by Influential Applications. INFORMS, 2013. p. 151-167.</p>
</li><li id="#Safe"><p>Citation to give for Robust linear prediction</p>
</li></ul><p style="text-align: right; font-size: 10px;"><p>Go to top</p>
</p>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Y. LAGUEL, J. MALICK, Z. HARCHAOUI

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>